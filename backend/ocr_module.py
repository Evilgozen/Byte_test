# -*- coding: utf-8 -*-
"""
OCRËØÜÂà´Ê®°Âùó
Ë¥üË¥£ËßÜÈ¢ëÂ∏ßÁöÑOCRËØÜÂà´„ÄÅÂÖ≥ÈîÆËØçÂàÜÊûêÁ≠âÂäüËÉΩ
"""

from fastapi import HTTPException, Depends
from sqlalchemy.orm import Session
from models_simple import Video, VideoFrame, OCRResult, StageConfig, ProcessStatus
from pathlib import Path
from typing import List, Optional, Dict, Any
from pydantic import BaseModel
from datetime import datetime
from paddleocr import PaddleOCR, TextRecognition
from config import OCRConfig
import json
import os
import time
import cv2


class OCRProcessRequest(BaseModel):
    use_gpu: Optional[bool] = False
    lang: Optional[str] = "ch"  # ËØ≠Ë®ÄÔºöch, enÁ≠â


class OCRResultResponse(BaseModel):
    id: int
    frame_id: int
    text_content: Optional[str]
    confidence: Optional[float]
    bbox: Optional[str]  # Store as JSON string to avoid serialization issues
    processed_at: datetime
    
    class Config:
        from_attributes = True


class EnhancedOCRResultResponse(BaseModel):
    """Â¢ûÂº∫ÁöÑOCRÁªìÊûúÂìçÂ∫îÊ®°Âûã - ÊîØÊåÅPP-OCRv5"""
    frame_id: int
    frame_path: str
    ocr_version: str
    processing_time: float
    text_blocks: List[Dict[str, Any]]
    full_text: str
    total_confidence: float
    text_count: int
    language: str
    image_info: Dict[str, Any]
    detection_results: List[Dict[str, Any]]
    recognition_results: List[Dict[str, Any]]
    
    class Config:
        from_attributes = True


class KeywordAnalysisRequest(BaseModel):
    keywords: List[str]
    analysis_type: Optional[str] = "appearance"  # appearance, disappearance, both


class KeywordAnalysisResponse(BaseModel):
    keyword: str
    first_appearance_timestamp: Optional[int]
    first_disappearance_timestamp: Optional[int]
    total_occurrences: int
    frame_occurrences: List[dict]


class OCRProcessor:
    """OCRÂ§ÑÁêÜÂô®"""
    
    def __init__(self):
        self.ocr_instance = None
        self.ocr_results_path = "./data/ocr_results"
        
        # Á°Æ‰øùOCRÁªìÊûúÂ≠òÂÇ®ÁõÆÂΩïÂ≠òÂú®
        Path(self.ocr_results_path).mkdir(parents=True, exist_ok=True)
        
        # ÂàùÂßãÂåñOCRÂÆû‰æã
        self.initialize_ocr()
    
    def initialize_ocr(self, use_gpu: bool = False, lang: str = "ch") -> None:
        """ÂàùÂßãÂåñOCRÂÆû‰æã - ‰ºòÂåñÁöÑPaddleOCRÈÖçÁΩÆ"""
        try:
            # ÂÖàÂ∞ùËØï‰ΩøÁî®Êñ∞ÁâàTextRecognition API
            try:
                self.text_recognition = TextRecognition()
                print("‚úì TextRecognitionÊ®°ÂûãÂàùÂßãÂåñÊàêÂäü")
            except Exception as te:
                print(f"‚ö† TextRecognitionÂàùÂßãÂåñÂ§±Ë¥•: {te}")
                self.text_recognition = None
            
            # ‰ΩøÁî®‰ºòÂåñÁöÑPaddleOCRÈÖçÁΩÆ
            self.ocr_instance = PaddleOCR(
                use_angle_cls=True, 
                lang='ch',
                show_log=False,  # ÂáèÂ∞ëÊó•ÂøóËæìÂá∫
                use_gpu=use_gpu
            )
            print("‚úì PaddleOCRÈÖçÁΩÆÂàùÂßãÂåñÊàêÂäü")
            
        except Exception as e:
            print(f"‚ö† OCRÂàùÂßãÂåñÂ§±Ë¥•: {e}")
            # ÊúÄÂü∫Êú¨ÁöÑÈÖçÁΩÆ
            self.text_recognition = None
            self.ocr_instance = PaddleOCR(use_angle_cls=True, lang='ch')
            print("‚úì ‰ΩøÁî®Âü∫Á°ÄPaddleOCRÈÖçÁΩÆ")
    
    def process_frame_ocr(self, frame_path: str, frame_id: int, use_gpu: bool = False, lang: str = "ch") -> dict:
        """ÂØπÂçï‰∏™Â∏ßËøõË°åOCRËØÜÂà´"""
        if not self.ocr_instance:
            raise ValueError("OCRÂÆû‰æãÊú™ÂàùÂßãÂåñ")
        
        if not os.path.exists(frame_path):
            raise ValueError(f"Â∏ßÂõæÁâáÊñá‰ª∂‰∏çÂ≠òÂú®: {frame_path}")
        
        try:
            # Ëé∑ÂèñÂõæÂÉè‰ø°ÊÅØ
            image = cv2.imread(frame_path)
            if image is None:
                print(f"‚ö† Êó†Ê≥ïËØªÂèñÂõæÂÉèÊñá‰ª∂: {frame_path}")
                raise ValueError(f"Êó†Ê≥ïËØªÂèñÂõæÂÉèÊñá‰ª∂: {frame_path}")
            
            image_height, image_width = image.shape[:2]
            image_channels = image.shape[2] if len(image.shape) > 2 else None
            print(f"üì∑ ÂõæÂÉè‰ø°ÊÅØ: {image_width}x{image_height}, ÈÄöÈÅìÊï∞: {image_channels}")
            
            # ËÆ∞ÂΩïÂ§ÑÁêÜÂºÄÂßãÊó∂Èó¥
            start_time = time.time()
            
            # ÊâßË°åOCRËØÜÂà´ - ‰ΩøÁî®ÊóßÁâàAPIÔºàÊõ¥Á®≥ÂÆöÔºâ
            print(f"üîç ÂºÄÂßãOCRËØÜÂà´: {frame_path}")
            result = self.ocr_instance.ocr(frame_path)
            print(f"üìù OCRÂéüÂßãÁªìÊûú: {result}")
            
            # ËÆ°ÁÆóÂ§ÑÁêÜÊó∂Èó¥
            processing_time = time.time() - start_time
            
            # Â§ÑÁêÜOCRÁªìÊûú - Â¢ûÂº∫ÁöÑJSONÁªìÊûÑ
            ocr_data = {
                "frame_id": frame_id,
                "frame_path": frame_path,
                "ocr_version": "PP-OCRv5",
                "processing_time": round(processing_time, 3),
                "text_blocks": [],
                "full_text": "",
                "total_confidence": 0.0,
                "text_count": 0,
                "language": lang,
                "image_info": {
                    "width": image_width,
                    "height": image_height,
                    "channels": image_channels
                },
                "detection_results": [],
                "recognition_results": []
            }
            
            # Â§ÑÁêÜ‰∏çÂêåÊ†ºÂºèÁöÑOCRÁªìÊûú
            if result:
                text_blocks = []
                confidences = []
                full_text_parts = []
                detection_results = []
                recognition_results = []
                
                # Ê£ÄÊü•ÊòØÂê¶ÊòØÊñ∞ÁâàTextRecognition APIÁöÑÁªìÊûúÊ†ºÂºè
                if isinstance(result, list) and len(result) > 0 and isinstance(result[0], dict):
                    # Êñ∞ÁâàAPIÊ†ºÂºèÂ§ÑÁêÜ
                    result_dict = result[0]
                    if 'rec_texts' in result_dict and 'rec_scores' in result_dict:
                        rec_texts = result_dict['rec_texts']
                        rec_scores = result_dict['rec_scores']
                        
                        for idx, (text, score) in enumerate(zip(rec_texts, rec_scores)):
                            if text.strip():  # Âè™Â§ÑÁêÜÈùûÁ©∫ÊñáÊú¨
                                # Ê®°ÊãüËæπÁïåÊ°ÜÔºàÊñ∞ÁâàAPIÂèØËÉΩ‰∏çÊèê‰æõËØ¶ÁªÜÂùêÊ†áÔºâ
                                bbox = [[0, idx*20], [100, idx*20], [100, (idx+1)*20], [0, (idx+1)*20]]
                                
                                text_block = {
                                    "id": idx,
                                    "text": text,
                                    "confidence": float(score),
                                    "bbox": bbox,
                                    "bbox_normalized": {
                                        "x1": 0,
                                        "y1": idx*20,
                                        "x2": 100,
                                        "y2": (idx+1)*20
                                    },
                                    "text_length": len(text),
                                    "word_count": len(text.split()) if text.strip() else 0
                                }
                                
                                detection_result = {
                                    "id": idx,
                                    "bbox": bbox,
                                    "confidence": float(score)
                                }
                                
                                recognition_result = {
                                    "id": idx,
                                    "text": text,
                                    "confidence": float(score)
                                }
                                
                                text_blocks.append(text_block)
                                detection_results.append(detection_result)
                                recognition_results.append(recognition_result)
                                confidences.append(float(score))
                                full_text_parts.append(text)
                elif isinstance(result, list) and len(result) > 0 and isinstance(result[0], list):
                     # ÊóßÁâàAPIÊ†ºÂºèÂ§ÑÁêÜ
                     for idx, line in enumerate(result[0]):
                         if len(line) >= 2:
                             bbox = line[0]  # ËæπÁïåÊ°ÜÂùêÊ†á
                             text_info = line[1]  # ÊñáÊú¨ÂíåÁΩÆ‰ø°Â∫¶
                             
                             if isinstance(text_info, (list, tuple)) and len(text_info) >= 2:
                                 text = text_info[0]
                                 confidence = float(text_info[1])
                                 
                                 # ËØ¶ÁªÜÁöÑÊñáÊú¨Âùó‰ø°ÊÅØ
                                 text_block = {
                                     "id": idx,
                                     "text": text,
                                     "confidence": confidence,
                                     "bbox": bbox,
                                     "bbox_normalized": {
                                         "x1": min([point[0] for point in bbox]),
                                         "y1": min([point[1] for point in bbox]),
                                         "x2": max([point[0] for point in bbox]),
                                         "y2": max([point[1] for point in bbox])
                                     },
                                     "text_length": len(text),
                                     "word_count": len(text.split()) if text.strip() else 0
                                 }
                                 
                                 # Ê£ÄÊµãÁªìÊûú
                                 detection_result = {
                                     "id": idx,
                                     "bbox": bbox,
                                     "confidence": confidence
                                 }
                                 
                                 # ËØÜÂà´ÁªìÊûú
                                 recognition_result = {
                                     "id": idx,
                                     "text": text,
                                     "confidence": confidence,
                                     "char_confidences": []  # ÂèØ‰ª•Êâ©Â±ï‰∏∫Â≠óÁ¨¶Á∫ßÁΩÆ‰ø°Â∫¶
                                 }
                                 
                                 text_blocks.append(text_block)
                                 detection_results.append(detection_result)
                                 recognition_results.append(recognition_result)
                                 confidences.append(confidence)
                                 full_text_parts.append(text)
                
                ocr_data["text_blocks"] = text_blocks
                ocr_data["detection_results"] = detection_results
                ocr_data["recognition_results"] = recognition_results
                ocr_data["full_text"] = " ".join(full_text_parts)
                ocr_data["total_confidence"] = sum(confidences) / len(confidences) if confidences else 0.0
                ocr_data["text_count"] = len(text_blocks)
            
            return ocr_data
            
        except Exception as e:
            raise ValueError(f"OCRÂ§ÑÁêÜÂ§±Ë¥•: {str(e)}")
    
    def _convert_new_api_result(self, output, frame_path: str):
        """ËΩ¨Êç¢Êñ∞ÁâàAPIÁªìÊûú‰∏∫ÊóßÁâàÊ†ºÂºè"""
        try:
            # ‰øùÂ≠òOCRÂ§ÑÁêÜÂêéÁöÑÂõæÁâáÂíåJSON
            output_dir = Path(f"{self.ocr_results_path}/ocr_output")
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ‰øùÂ≠òÁªìÊûúÂõæÁâáÂíåJSON
            for res in output:
                res.save_to_img(save_path=str(output_dir))
                res.save_to_json(save_path=str(output_dir / "result.json"))
            
            # ËΩ¨Êç¢‰∏∫ÊóßÁâàÊ†ºÂºè - ËøôÈáåÈúÄË¶ÅÊ†πÊçÆÊñ∞ÁâàAPIÁöÑÂÆûÈôÖËæìÂá∫ÁªìÊûÑË∞ÉÊï¥
            converted_result = []
            for res in output:
                # Êñ∞ÁâàAPIÁöÑÁªìÊûúÁªìÊûÑÂèØËÉΩ‰∏çÂêåÔºåËøôÈáåÂÅöÂü∫Êú¨ËΩ¨Êç¢
                # ÂÖ∑‰ΩìËΩ¨Êç¢ÈÄªËæëÈúÄË¶ÅÊ†πÊçÆÂÆûÈôÖAPIËæìÂá∫Ë∞ÉÊï¥
                if hasattr(res, 'text') and hasattr(res, 'confidence'):
                    converted_result.append([
                        [[0, 0], [100, 0], [100, 30], [0, 30]],  # ÈªòËÆ§ËæπÁïåÊ°Ü
                        [res.text, res.confidence]
                    ])
            
            return [converted_result] if converted_result else [[]]
            
        except Exception as e:
            print(f"Êñ∞ÁâàAPIÁªìÊûúËΩ¨Êç¢Â§±Ë¥•: {e}")
            return [[]]
    
    def save_ocr_result_to_file(self, video_id: int, frame_number: int, ocr_data: dict) -> str:
        """‰øùÂ≠òOCRÁªìÊûúÂà∞JSONÊñá‰ª∂"""
        ocr_output_dir = Path(f"{self.ocr_results_path}/video_{video_id}")
        ocr_output_dir.mkdir(parents=True, exist_ok=True)
        
        ocr_json_path = ocr_output_dir / f"frame_{frame_number}_ocr.json"
        with open(ocr_json_path, 'w', encoding='utf-8') as f:
            json.dump(ocr_data, f, ensure_ascii=False, indent=2)
        
        return str(ocr_json_path)
    
    async def process_video_ocr(self, video_id: int, request: OCRProcessRequest, db: Session) -> dict:
        """ÂØπËßÜÈ¢ëÁöÑÊâÄÊúâÂ∏ßËøõË°åOCRÂ§ÑÁêÜ"""
        # Ê£ÄÊü•ËßÜÈ¢ëÊòØÂê¶Â≠òÂú®
        video = db.query(Video).filter(Video.id == video_id).first()
        if not video:
            raise HTTPException(status_code=404, detail="ËßÜÈ¢ë‰∏çÂ≠òÂú®")
        
        # Ëé∑ÂèñËßÜÈ¢ëÁöÑÊâÄÊúâÂ∏ß
        frames = db.query(VideoFrame).filter(VideoFrame.video_id == video_id).order_by(VideoFrame.frame_number).all()
        if not frames:
            raise HTTPException(status_code=404, detail="ËßÜÈ¢ëÂ∏ß‰∏çÂ≠òÂú®ÔºåËØ∑ÂÖàËøõË°åÂàÜÂ∏ßÂ§ÑÁêÜ")
        
        try:
            # Êõ¥Êñ∞ËßÜÈ¢ëÁä∂ÊÄÅ‰∏∫Â§ÑÁêÜ‰∏≠
            video.process_status = ProcessStatus.processing
            db.commit()
            
            processed_frames = 0
            failed_frames = 0
            ocr_results = []
            
            for frame in frames:
                try:
                    print(f"üé¨ Â§ÑÁêÜÂ∏ß: {frame.id}, Ë∑ØÂæÑ: {frame.frame_path}")
                    
                    # Ê£ÄÊü•ÊòØÂê¶Â∑≤ÁªèÂ§ÑÁêÜËøáOCR
                    existing_ocr = db.query(OCRResult).filter(OCRResult.frame_id == frame.id).first()
                    if existing_ocr:
                        print(f"‚è≠ Ë∑≥ËøáÂ∑≤Â§ÑÁêÜÁöÑÂ∏ß: {frame.id}")
                        continue
                    
                    # Â§ÑÁêÜOCR
                    print(f"üîç ÂºÄÂßãÂ§ÑÁêÜÂ∏ß {frame.id} ÁöÑOCR")
                    ocr_data = self.process_frame_ocr(frame.frame_path, frame.id, request.use_gpu, request.lang)
                    print(f"‚úÖ Â∏ß {frame.id} OCRÂ§ÑÁêÜÂÆåÊàêÔºåÊñáÊú¨Êï∞Èáè: {ocr_data.get('text_count', 0)}")
                    
                    # ‰øùÂ≠òOCRÁªìÊûúÂà∞Êï∞ÊçÆÂ∫ì
                    db_ocr = OCRResult(
                        frame_id=frame.id,
                        text_content=ocr_data["full_text"],
                        confidence=ocr_data["total_confidence"],
                        bbox=json.dumps(ocr_data["text_blocks"], ensure_ascii=False)
                    )
                    
                    db.add(db_ocr)
                    processed_frames += 1
                    
                    # ‰øùÂ≠òOCRÁªìÊûúÂà∞JSONÊñá‰ª∂
                    self.save_ocr_result_to_file(video_id, frame.frame_number, ocr_data)
                    
                    ocr_results.append({
                        "frame_id": frame.id,
                        "frame_number": frame.frame_number,
                        "timestamp_ms": frame.timestamp_ms,
                        "text_content": ocr_data["full_text"],
                        "confidence": ocr_data["total_confidence"],
                        "text_blocks_count": len(ocr_data["text_blocks"])
                    })
                    
                except Exception as e:
                    failed_frames += 1
                    print(f"Â§ÑÁêÜÂ∏ß {frame.id} OCRÂ§±Ë¥•: {e}")
            
            # ÊâπÈáèÊèê‰∫§Êï∞ÊçÆÂ∫ìÊõ¥Êîπ
            db.commit()
            
            # Êõ¥Êñ∞ËßÜÈ¢ëÁä∂ÊÄÅ‰∏∫ÂÆåÊàê
            video.process_status = ProcessStatus.completed
            db.commit()
            
            return {
                "message": "OCRÂ§ÑÁêÜÂÆåÊàê",
                "video_id": video_id,
                "total_frames": len(frames),
                "processed_frames": processed_frames,
                "failed_frames": failed_frames,
                "ocr_results": ocr_results[:10]  # Âè™ËøîÂõûÂâç10‰∏™ÁªìÊûú‰Ωú‰∏∫Á§∫‰æã
            }
            
        except Exception as e:
            # Êõ¥Êñ∞ËßÜÈ¢ëÁä∂ÊÄÅ‰∏∫Â§±Ë¥•
            video.process_status = ProcessStatus.failed
            db.commit()
            raise HTTPException(status_code=500, detail=f"OCRÂ§ÑÁêÜÂ§±Ë¥•: {str(e)}")
    
    def get_video_ocr_results(self, video_id: int, db: Session) -> List[OCRResult]:
        """Ëé∑ÂèñËßÜÈ¢ëÁöÑÊâÄÊúâOCRÁªìÊûú"""
        # Ê£ÄÊü•ËßÜÈ¢ëÊòØÂê¶Â≠òÂú®
        video = db.query(Video).filter(Video.id == video_id).first()
        if not video:
            raise HTTPException(status_code=404, detail="ËßÜÈ¢ë‰∏çÂ≠òÂú®")
        
        # Ëé∑ÂèñOCRÁªìÊûú
        ocr_results = db.query(OCRResult).join(
            VideoFrame, OCRResult.frame_id == VideoFrame.id
        ).filter(
            VideoFrame.video_id == video_id
        ).order_by(VideoFrame.timestamp_ms).all()
        
        return ocr_results
    
    def get_enhanced_ocr_results(self, video_id: int) -> List[EnhancedOCRResultResponse]:
        """Ëé∑ÂèñÂ¢ûÂº∫ÁöÑOCRÁªìÊûúÔºà‰ªéJSONÊñá‰ª∂ËØªÂèñÔºâ"""
        try:
            enhanced_results = []
            ocr_results_dir = os.path.join("data", "ocr_results")
            
            # Êü•ÊâæËØ•ËßÜÈ¢ëÁöÑÊâÄÊúâOCRÁªìÊûúÊñá‰ª∂
            for filename in os.listdir(ocr_results_dir):
                if filename.startswith(f"video_{video_id}_frame_") and filename.endswith(".json"):
                    file_path = os.path.join(ocr_results_dir, filename)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            ocr_data = json.load(f)
                            enhanced_results.append(EnhancedOCRResultResponse(**ocr_data))
                    except Exception as e:
                        print(f"ËØªÂèñOCRÁªìÊûúÊñá‰ª∂Â§±Ë¥• {filename}: {e}")
                        continue
            
            # Êåâframe_idÊéíÂ∫è
            enhanced_results.sort(key=lambda x: x.frame_id)
            return enhanced_results
            
        except Exception as e:
            print(f"Ëé∑ÂèñÂ¢ûÂº∫OCRÁªìÊûúÂ§±Ë¥•: {e}")
            raise HTTPException(status_code=500, detail="Ëé∑ÂèñÂ¢ûÂº∫OCRÁªìÊûúÂ§±Ë¥•")
    
    def get_frame_ocr_result(self, frame_id: int, db: Session) -> OCRResult:
        """Ëé∑ÂèñÊåáÂÆöÂ∏ßÁöÑOCRÁªìÊûú"""
        ocr_result = db.query(OCRResult).filter(OCRResult.frame_id == frame_id).first()
        if not ocr_result:
            raise HTTPException(status_code=404, detail="Â∏ßOCRÁªìÊûú‰∏çÂ≠òÂú®")
        return ocr_result
    
    def analyze_keywords_in_ocr_results(self, video_id: int, keywords: List[str], db: Session) -> List[dict]:
        """ÂàÜÊûêOCRÁªìÊûú‰∏≠ÁöÑÂÖ≥ÈîÆËØçÂá∫Áé∞ÂíåÊ∂àÂ§±Ê®°Âºè"""
        # Ëé∑ÂèñËßÜÈ¢ëÁöÑÊâÄÊúâÂ∏ßÂíåOCRÁªìÊûú
        frames_with_ocr = db.query(VideoFrame, OCRResult).join(
            OCRResult, VideoFrame.id == OCRResult.frame_id
        ).filter(
            VideoFrame.video_id == video_id
        ).order_by(VideoFrame.timestamp_ms).all()
        
        if not frames_with_ocr:
            return []
        
        # ÂàÜÊûêÊØè‰∏™ÂÖ≥ÈîÆËØç
        analysis_results = []
        
        for keyword in keywords:
            keyword_analysis = {
                "keyword": keyword,
                "first_appearance_timestamp": None,
                "first_disappearance_timestamp": None,
                "total_occurrences": 0,
                "frame_occurrences": [],
                "pattern_analysis": {
                    "continuous_periods": [],
                    "gap_periods": []
                }
            }
            
            previous_found = False
            current_period_start = None
            
            for frame, ocr_result in frames_with_ocr:
                # Ê£ÄÊü•ÂÖ≥ÈîÆËØçÊòØÂê¶Âú®OCRÊñáÊú¨‰∏≠
                text_content = ocr_result.text_content or ""
                found_in_frame = keyword.lower() in text_content.lower()
                
                if found_in_frame:
                    keyword_analysis["total_occurrences"] += 1
                    keyword_analysis["frame_occurrences"].append({
                        "frame_id": frame.id,
                        "timestamp_ms": frame.timestamp_ms,
                        "confidence": float(ocr_result.confidence) if ocr_result.confidence else 0.0
                    })
                    
                    # ËÆ∞ÂΩïÁ¨¨‰∏ÄÊ¨°Âá∫Áé∞
                    if keyword_analysis["first_appearance_timestamp"] is None:
                        keyword_analysis["first_appearance_timestamp"] = frame.timestamp_ms
                    
                    # ÂºÄÂßãËøûÁª≠ÊúüÈó¥
                    if not previous_found:
                        current_period_start = frame.timestamp_ms
                
                else:
                    # ÂÖ≥ÈîÆËØçÊ∂àÂ§±
                    if previous_found:
                        # ËÆ∞ÂΩïÁ¨¨‰∏ÄÊ¨°Ê∂àÂ§±
                        if keyword_analysis["first_disappearance_timestamp"] is None:
                            keyword_analysis["first_disappearance_timestamp"] = frame.timestamp_ms
                        
                        # ÁªìÊùüËøûÁª≠ÊúüÈó¥
                        if current_period_start is not None:
                            keyword_analysis["pattern_analysis"]["continuous_periods"].append({
                                "start_timestamp": current_period_start,
                                "end_timestamp": frame.timestamp_ms,
                                "duration_ms": frame.timestamp_ms - current_period_start
                            })
                            current_period_start = None
                
                previous_found = found_in_frame
            
            # Â§ÑÁêÜÊúÄÂêé‰∏Ä‰∏™ËøûÁª≠ÊúüÈó¥
            if current_period_start is not None and frames_with_ocr:
                last_frame = frames_with_ocr[-1][0]
                keyword_analysis["pattern_analysis"]["continuous_periods"].append({
                    "start_timestamp": current_period_start,
                    "end_timestamp": last_frame.timestamp_ms,
                    "duration_ms": last_frame.timestamp_ms - current_period_start
                })
            
            analysis_results.append(keyword_analysis)
        
        return analysis_results
    
    async def analyze_video_keywords(self, video_id: int, request: KeywordAnalysisRequest, db: Session) -> dict:
        """ÂàÜÊûêËßÜÈ¢ë‰∏≠ÂÖ≥ÈîÆËØçÁöÑÂá∫Áé∞ÂíåÊ∂àÂ§±Ê®°Âºè"""
        # Ê£ÄÊü•ËßÜÈ¢ëÊòØÂê¶Â≠òÂú®
        video = db.query(Video).filter(Video.id == video_id).first()
        if not video:
            raise HTTPException(status_code=404, detail="ËßÜÈ¢ë‰∏çÂ≠òÂú®")
        
        # Ê£ÄÊü•ÊòØÂê¶ÊúâOCRÁªìÊûú
        ocr_count = db.query(OCRResult).join(
            VideoFrame, OCRResult.frame_id == VideoFrame.id
        ).filter(
            VideoFrame.video_id == video_id
        ).count()
        
        if ocr_count == 0:
            raise HTTPException(status_code=404, detail="ËßÜÈ¢ëOCRÁªìÊûú‰∏çÂ≠òÂú®ÔºåËØ∑ÂÖàËøõË°åOCRÂ§ÑÁêÜ")
        
        try:
            # ÊâßË°åÂÖ≥ÈîÆËØçÂàÜÊûê
            analysis_results = self.analyze_keywords_in_ocr_results(video_id, request.keywords, db)
            
            return {
                "message": "ÂÖ≥ÈîÆËØçÂàÜÊûêÂÆåÊàê",
                "video_id": video_id,
                "analyzed_keywords": len(request.keywords),
                "analysis_results": analysis_results
            }
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"ÂÖ≥ÈîÆËØçÂàÜÊûêÂ§±Ë¥•: {str(e)}")
    
    async def analyze_stage_keywords(self, video_id: int, db: Session) -> dict:
        """Âü∫‰∫éstage_configsÂàÜÊûêÂÖ≥ÈîÆËØçÊ®°Âºè"""
        # Ê£ÄÊü•ËßÜÈ¢ëÊòØÂê¶Â≠òÂú®
        video = db.query(Video).filter(Video.id == video_id).first()
        if not video:
            raise HTTPException(status_code=404, detail="ËßÜÈ¢ë‰∏çÂ≠òÂú®")
        
        # Ëé∑ÂèñËßÜÈ¢ëÁöÑÈò∂ÊÆµÈÖçÁΩÆ
        stage_configs = db.query(StageConfig).filter(
            StageConfig.video_id == video_id
        ).order_by(StageConfig.stage_order).all()
        
        if not stage_configs:
            raise HTTPException(status_code=404, detail="ËßÜÈ¢ëÈò∂ÊÆµÈÖçÁΩÆ‰∏çÂ≠òÂú®")
        
        # Ê£ÄÊü•ÊòØÂê¶ÊúâOCRÁªìÊûú
        ocr_count = db.query(OCRResult).join(
            VideoFrame, OCRResult.frame_id == VideoFrame.id
        ).filter(
            VideoFrame.video_id == video_id
        ).count()
        
        if ocr_count == 0:
            raise HTTPException(status_code=404, detail="ËßÜÈ¢ëOCRÁªìÊûú‰∏çÂ≠òÂú®ÔºåËØ∑ÂÖàËøõË°åOCRÂ§ÑÁêÜ")
        
        try:
            stage_analysis_results = []
            
            for config in stage_configs:
                # Ëß£ÊûêÂÖ≥ÈîÆËØç
                keywords = json.loads(config.keywords) if isinstance(config.keywords, str) else config.keywords
                
                # ÂàÜÊûêÂÖ≥ÈîÆËØç
                analysis_results = self.analyze_keywords_in_ocr_results(video_id, keywords, db)
                
                stage_analysis_results.append({
                    "stage_id": config.id,
                    "stage_name": config.stage_name,
                    "stage_order": config.stage_order,
                    "keywords": keywords,
                    "keyword_analysis": analysis_results
                })
            
            return {
                "message": "Èò∂ÊÆµÂÖ≥ÈîÆËØçÂàÜÊûêÂÆåÊàê",
                "video_id": video_id,
                "total_stages": len(stage_configs),
                "stage_analysis_results": stage_analysis_results
            }
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Èò∂ÊÆµÂÖ≥ÈîÆËØçÂàÜÊûêÂ§±Ë¥•: {str(e)}")
    
    async def analyze_keywords_by_stage_config(self, config_id: int, db: Session) -> dict:
        """Âü∫‰∫éÈò∂ÊÆµÈÖçÁΩÆËøõË°åÂÖ≥ÈîÆËØçÂàÜÊûê"""
        try:
            # Ëé∑ÂèñÈò∂ÊÆµÈÖçÁΩÆ
            stage_config = db.query(StageConfig).filter(StageConfig.id == config_id).first()
            if not stage_config:
                raise HTTPException(status_code=404, detail="Èò∂ÊÆµÈÖçÁΩÆ‰∏çÂ≠òÂú®")
            
            # Ëß£ÊûêÂÖ≥ÈîÆËØç
            keywords = json.loads(stage_config.keywords) if isinstance(stage_config.keywords, str) else stage_config.keywords
            if not keywords:
                raise HTTPException(status_code=400, detail="Èò∂ÊÆµÈÖçÁΩÆ‰∏≠Ê≤°ÊúâÂÖ≥ÈîÆËØç")
            
            # ÊâßË°åÂÖ≥ÈîÆËØçÂàÜÊûê
            analysis_results = self.analyze_keywords_in_ocr_results(stage_config.video_id, keywords, db)
            
            return {
                "message": "Âü∫‰∫éÈò∂ÊÆµÈÖçÁΩÆÁöÑÂÖ≥ÈîÆËØçÂàÜÊûêÂÆåÊàê",
                "stage_config_id": config_id,
                "video_id": stage_config.video_id,
                "stage_name": stage_config.stage_name,
                "keywords_count": len(keywords),
                "analysis_results": analysis_results
            }
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"ÂÖ≥ÈîÆËØçÂàÜÊûêÂ§±Ë¥•: {str(e)}")
    
    def delete_video_ocr_results(self, video_id: int, db: Session) -> dict:
        """Âà†Èô§ËßÜÈ¢ëÁöÑÊâÄÊúâOCRÁªìÊûúÔºàÊï∞ÊçÆÂ∫ìËÆ∞ÂΩïÂíåJSONÊñá‰ª∂Ôºâ"""
        try:
            # Ê£ÄÊü•ËßÜÈ¢ëÊòØÂê¶Â≠òÂú®
            video = db.query(Video).filter(Video.id == video_id).first()
            if not video:
                raise HTTPException(status_code=404, detail="ËßÜÈ¢ë‰∏çÂ≠òÂú®")
            
            # Ëé∑ÂèñËßÜÈ¢ëÁöÑÊâÄÊúâÂ∏ß
            frames = db.query(VideoFrame).filter(VideoFrame.video_id == video_id).all()
            
            deleted_db_records = 0
            deleted_json_files = 0
            
            # Âà†Èô§Êï∞ÊçÆÂ∫ì‰∏≠ÁöÑOCRËÆ∞ÂΩï
            for frame in frames:
                ocr_results = db.query(OCRResult).filter(OCRResult.frame_id == frame.id).all()
                for ocr_result in ocr_results:
                    db.delete(ocr_result)
                    deleted_db_records += 1
            
            # Êèê‰∫§Êï∞ÊçÆÂ∫ìÊõ¥Êîπ
            db.commit()
            
            # Âà†Èô§JSONÊñá‰ª∂
            ocr_output_dir = Path(f"{self.ocr_results_path}/video_{video_id}")
            if ocr_output_dir.exists():
                import shutil
                for json_file in ocr_output_dir.glob("*.json"):
                    try:
                        json_file.unlink()
                        deleted_json_files += 1
                    except Exception as e:
                        print(f"Âà†Èô§JSONÊñá‰ª∂Â§±Ë¥•: {json_file}, ÈîôËØØ: {e}")
                
                # Â¶ÇÊûúÁõÆÂΩï‰∏∫Á©∫ÔºåÂà†Èô§ÁõÆÂΩï
                try:
                    if not any(ocr_output_dir.iterdir()):
                        ocr_output_dir.rmdir()
                except Exception as e:
                    print(f"Âà†Èô§ÁõÆÂΩïÂ§±Ë¥•: {ocr_output_dir}, ÈîôËØØ: {e}")
            
            return {
                "message": "OCRÁªìÊûúÂà†Èô§ÊàêÂäü",
                "video_id": video_id,
                "deleted_db_records": deleted_db_records,
                "deleted_json_files": deleted_json_files
            }
            
        except Exception as e:
            db.rollback()
            raise HTTPException(status_code=500, detail=f"Âà†Èô§OCRÁªìÊûúÂ§±Ë¥•: {str(e)}")
    
    def get_ocr_storage_info(self, video_id: int, db: Session) -> dict:
        """Ëé∑ÂèñOCRÁªìÊûúÁöÑÂ≠òÂÇ®‰ø°ÊÅØ"""
        try:
            # Ê£ÄÊü•ËßÜÈ¢ëÊòØÂê¶Â≠òÂú®
            video = db.query(Video).filter(Video.id == video_id).first()
            if not video:
                raise HTTPException(status_code=404, detail="ËßÜÈ¢ë‰∏çÂ≠òÂú®")
            
            # ÁªüËÆ°Êï∞ÊçÆÂ∫ì‰∏≠ÁöÑOCRËÆ∞ÂΩï
            frames = db.query(VideoFrame).filter(VideoFrame.video_id == video_id).all()
            total_frames = len(frames)
            
            db_ocr_count = 0
            for frame in frames:
                ocr_count = db.query(OCRResult).filter(OCRResult.frame_id == frame.id).count()
                db_ocr_count += ocr_count
            
            # Ê£ÄÊü•JSONÊñá‰ª∂Â≠òÂÇ®
            ocr_output_dir = Path(f"{self.ocr_results_path}/video_{video_id}")
            json_files = []
            total_json_size = 0
            
            if ocr_output_dir.exists():
                for json_file in ocr_output_dir.glob("*.json"):
                    file_size = json_file.stat().st_size
                    json_files.append({
                        "filename": json_file.name,
                        "size_bytes": file_size,
                        "size_kb": round(file_size / 1024, 2)
                    })
                    total_json_size += file_size
            
            return {
                "video_id": video_id,
                "database_info": {
                    "total_frames": total_frames,
                    "ocr_records_count": db_ocr_count
                },
                "json_storage_info": {
                    "storage_path": str(ocr_output_dir),
                    "json_files_count": len(json_files),
                    "total_size_bytes": total_json_size,
                    "total_size_kb": round(total_json_size / 1024, 2),
                    "files": json_files[:10]  # Âè™ÊòæÁ§∫Ââç10‰∏™Êñá‰ª∂
                }
            }
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Ëé∑ÂèñOCRÂ≠òÂÇ®‰ø°ÊÅØÂ§±Ë¥•: {str(e)}")


# ÂàõÂª∫ÂÖ®Â±ÄOCRÂ§ÑÁêÜÂô®ÂÆû‰æã
ocr_processor = OCRProcessor()